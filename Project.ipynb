{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4cad5f-49ca-436c-b14b-de46653f06a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 23:35:42,953 - INFO - Downloading NLTK resource 'vader_lexicon'...\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/abduladeeb/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "2024-12-08 23:35:43,098 - INFO - Downloading NLTK resource 'punkt'...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/abduladeeb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "2024-12-08 23:35:43,147 - INFO - NLTK resource 'stopwords' already exists.\n",
      "2024-12-08 23:35:43,153 - INFO - Downloading NLTK resource 'wordnet'...\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/abduladeeb/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "2024-12-08 23:35:43,284 - INFO - Downloading NLTK resource 'vader_lexicon'...\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/abduladeeb/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "2024-12-08 23:35:43,292 - INFO - Downloading NLTK resource 'punkt'...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/abduladeeb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "2024-12-08 23:35:43,310 - INFO - NLTK resource 'stopwords' already exists.\n",
      "2024-12-08 23:35:43,314 - INFO - Downloading NLTK resource 'wordnet'...\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/abduladeeb/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "2024-12-08 23:35:43,316 - INFO - Sentiment Analysis Pipeline Started.\n",
      "2024-12-08 23:35:43,329 - INFO - Loading dataset...\n",
      "2024-12-08 23:36:04,693 - INFO - After dropping missing 'review_text', 146393 samples remain.\n",
      "2024-12-08 23:36:04,713 - INFO - Preprocessing text data...\n",
      "2024-12-08 23:36:41,274 - INFO - Text preprocessing completed.\n",
      "2024-12-08 23:36:41,275 - INFO - Loading GloVe embeddings from cache: glove_cache.pkl\n",
      "2024-12-08 23:36:42,463 - INFO - Successfully loaded 400000 word vectors from cache.\n",
      "2024-12-08 23:36:42,466 - INFO - Creating document embeddings...\n",
      "2024-12-08 23:36:47,715 - INFO - Document embeddings created with shape: (146393, 300)\n",
      "2024-12-08 23:36:47,717 - INFO - Generating VADER sentiment scores...\n",
      "2024-12-08 23:37:41,190 - INFO - VADER sentiment scores generated.\n",
      "2024-12-08 23:37:41,194 - INFO - Integrating VADER scores with GloVe embeddings...\n",
      "2024-12-08 23:37:41,412 - INFO - Final feature set shape before scaling: (146393, 304)\n",
      "2024-12-08 23:37:41,413 - INFO - Applying StandardScaler to features...\n",
      "2024-12-08 23:37:41,814 - INFO - Feature scaling applied.\n",
      "2024-12-08 23:37:41,837 - INFO - Labels encoded.\n",
      "2024-12-08 23:37:41,868 - INFO - Splitting dataset into training and testing sets...\n",
      "2024-12-08 23:37:42,052 - INFO - Training set size: 117114\n",
      "2024-12-08 23:37:42,053 - INFO - Testing set size: 29279\n",
      "2024-12-08 23:37:42,053 - INFO - Applying SMOTE to balance classes...\n",
      "2024-12-08 23:37:43,707 - INFO - After SMOTE: 216024 training samples\n",
      "2024-12-08 23:37:43,714 - INFO - \n",
      "Processing LightGBM...\n",
      "2024-12-08 23:37:43,746 - INFO - Training LightGBM...\n",
      "2024-12-08 23:41:18,433 - INFO - LightGBM training completed.\n",
      "2024-12-08 23:41:18,436 - INFO - Evaluating LightGBM...\n",
      "2024-12-08 23:41:18,659 - INFO - \n",
      "LightGBM Classification Report:\n",
      "2024-12-08 23:41:18,685 - INFO -               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.32      0.50      0.39      2275\n",
      "    Positive       0.96      0.91      0.93     27004\n",
      "\n",
      "    accuracy                           0.88     29279\n",
      "   macro avg       0.64      0.71      0.66     29279\n",
      "weighted avg       0.91      0.88      0.89     29279\n",
      "\n",
      "2024-12-08 23:41:18,686 - INFO - Performing cross-validation for LightGBM...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import logging\n",
    "import pickle\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    StratifiedKFold\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "\n",
    "def output_logger(log_file: str = 'sentimentOutput.log') -> None:\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[logging.FileHandler(log_file), logging.StreamHandler()]\n",
    "    )\n",
    "\n",
    "\n",
    "def check_nltk() -> None:\n",
    "    nlkt_resources = ['vader_lexicon', 'punkt', 'stopwords', 'wordnet']\n",
    "    for resource in nlkt_resources:\n",
    "        try:\n",
    "            nltk.data.find(f'corpora/{resource}')\n",
    "            logging.info(f\"NLTK resource '{resource}' already exists.\")\n",
    "        except LookupError:\n",
    "            logging.info(f\"Downloading NLTK resource '{resource}'...\")\n",
    "            nltk.download(resource)\n",
    "\n",
    "\n",
    "def preprocess_text(text: str, lemmatizer: WordNetLemmatizer, stop_words: set) -> List[str]:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)      \n",
    "    text = re.sub(r'\\S+@\\S+', '', text)               \n",
    "    text = re.sub(r'<.*?>', '', text)                  \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'\\d+', '', text)                    \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens\n",
    "def load_glove_embeddings(cache_file: str = 'glove_cache.pkl') -> Dict[str, np.ndarray]:\n",
    "    if os.path.exists(cache_file):\n",
    "        logging.info(f\"Loading GloVe embeddings from cache: {cache_file}\")\n",
    "        try:\n",
    "            with open(cache_file, 'rb') as f:\n",
    "                embeddings_index = pickle.load(f)\n",
    "            logging.info(f\"Successfully loaded {len(embeddings_index)} word vectors from cache.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load cache file {cache_file}: {e}\")\n",
    "            raise\n",
    "    else:\n",
    "        logging.error(f\"Cache file not found at '{cache_file}'. Please ensure the cache exists.\")\n",
    "        raise FileNotFoundError(f\"Cache file '{cache_file}' does not exist.\")\n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "def get_document_embedding_glove(tokens: List[str], embeddings: Dict[str, np.ndarray], vector_size: int = 300) -> np.ndarray:\n",
    "    vectors = [embeddings[word] for word in tokens if word in embeddings]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "\n",
    "def evaluate_model(y_true: np.ndarray, y_pred: np.ndarray, model_name: str) -> None:\n",
    "    logging.info(f\"\\n{model_name} Classification Report:\")\n",
    "    report = classification_report(y_true, y_pred, target_names=['Negative', 'Positive'])\n",
    "    logging.info(report)\n",
    "\n",
    "\n",
    "def run_pipeline(data_path: str, glove_cache_path: str, output_dir: str = 'outputs') -> None:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_logger(os.path.join(output_dir, 'sentimentOutput.log'))\n",
    "    check_nltk()\n",
    "    logging.info(\"Sentiment Analysis Pipeline Started.\")\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    logging.info(\"Loading dataset...\")\n",
    "    data = pd.read_excel(data_path)\n",
    "    data = data.dropna(subset=['review_text'])\n",
    "    logging.info(f\"After dropping missing 'review_text', {data.shape[0]} samples remain.\")\n",
    "\n",
    "    data['sentiment'] = ['positive' if r >= 7 else 'negative' for r in data['rating']]\n",
    "    y = data['sentiment']\n",
    "    X = data['review_text']\n",
    "\n",
    "    logging.info(\"Preprocessing text data...\")\n",
    "    X_tokens = X.apply(lambda x: preprocess_text(x, lemmatizer, stop_words))\n",
    "    logging.info(\"Text preprocessing completed.\")\n",
    "\n",
    "    embeddings_index = load_glove_embeddings(cache_file=glove_cache_path)\n",
    "\n",
    "    logging.info(\"Creating document embeddings...\")\n",
    "    X_embedded = X_tokens.apply(lambda tokens: get_document_embedding_glove(tokens, embeddings_index))\n",
    "    X_embedded = np.vstack(X_embedded.values)\n",
    "    logging.info(f\"Document embeddings created with shape: {X_embedded.shape}\")\n",
    "\n",
    "    logging.info(\"Generating VADER sentiment scores...\")\n",
    "    vader_scores = X.apply(lambda x: sia.polarity_scores(x))\n",
    "    vader_scores_df = pd.DataFrame(list(vader_scores))\n",
    "    X_vader = vader_scores_df[['neg', 'neu', 'pos', 'compound']].values\n",
    "    logging.info(\"VADER sentiment scores generated.\")\n",
    "\n",
    "    logging.info(\"Integrating VADER scores with GloVe embeddings...\")\n",
    "    X_final = np.hstack((X_embedded, X_vader))\n",
    "    logging.info(f\"Final feature set shape before scaling: {X_final.shape}\")\n",
    "\n",
    "    logging.info(\"Applying StandardScaler to features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_final = scaler.fit_transform(X_final)\n",
    "    logging.info(\"Feature scaling applied.\")\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    logging.info(\"Labels encoded.\")\n",
    "\n",
    "    logging.info(\"Splitting dataset into training and testing sets...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_final, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "    logging.info(f\"Training set size: {X_train.shape[0]}\")\n",
    "    logging.info(f\"Testing set size: {X_test.shape[0]}\")\n",
    "\n",
    "    logging.info(\"Applying SMOTE to balance classes...\")\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "    logging.info(f\"After SMOTE: {X_train.shape[0]} training samples\")\n",
    "\n",
    "    models = {}\n",
    "    \n",
    "    models['LightGBM'] = LGBMClassifier(boosting_type='gbdt', learning_rate=0.1, max_depth=20, n_estimators=200, num_leaves=50, class_weight='balanced', random_state=42, verbose=-1)\n",
    "    \n",
    "    models['CatBoost'] = CatBoostClassifier(iterations=200, depth=10, learning_rate=0.1, l2_leaf_reg=3, class_weights=[1.0, 1.0], random_seed=42, verbose=0)\n",
    "    \n",
    "    models['XGBoost'] = XGBClassifier(scale_pos_weight=1.0, n_estimators=200, max_depth=10, learning_rate=0.1, subsample=0.8, colsample_bytree=0.8, objective='binary:logistic', use_label_encoder=False, eval_metric='logloss',random_state=4)\n",
    "    \n",
    "    models['BalancedRandomForest'] = BalancedRandomForestClassifier(n_estimators=200, max_depth=20, min_samples_split=2, min_samples_leaf=1, random_state=42)\n",
    "    \n",
    "    estimators = [('lgbm', models['LightGBM']), ('catboost', models['CatBoost'])]\n",
    "\n",
    "    models['StackingClassifier'] = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(), cv=5, n_jobs=-1)\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        logging.info(f\"\\nProcessing {model_name}...\")\n",
    "\n",
    "        logging.info(f\"Training {model_name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        logging.info(f\"{model_name} training completed.\")\n",
    "\n",
    "        logging.info(f\"Evaluating {model_name}...\")\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        else:\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_pred_proba = model.decision_function(X_test) if hasattr(model, 'decision_function') else y_pred\n",
    "        evaluate_model(y_test, y_pred, model_name)\n",
    "\n",
    "        logging.info(f\"Performing cross-validation for {model_name}...\")\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        cv_scores = cross_val_score(\n",
    "            model, X_final, y_encoded, cv=skf, scoring='f1_macro', n_jobs=-1\n",
    "        )\n",
    "        logging.info(f\"{model_name} Cross-Validated F1-Macro Scores: {cv_scores}\")\n",
    "        logging.info(f\"Average F1-Macro Score: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}\")\n",
    "\n",
    "    logging.info(\"\\nAll processes completed successfully!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_path = 'data/data.xlsx'          \n",
    "    glove_cache_path = 'glove_cache.pkl' \n",
    "    output_dir = 'outputs'                \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_logger(os.path.join(output_dir, 'sentimentOutput.log'))\n",
    "    check_nltk()\n",
    "    run_pipeline(data_path, glove_cache_path, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56145ca-e8eb-48d4-b08c-13b654487988",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
