{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4cad5f-49ca-436c-b14b-de46653f06a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import logging\n",
    "import pickle\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    StratifiedKFold\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "\n",
    "def output_logger(log_file: str = 'sentimentOutput.log') -> None:\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[logging.FileHandler(log_file), logging.StreamHandler()]\n",
    "    )\n",
    "\n",
    "\n",
    "def check_nltk() -> None:\n",
    "    nlkt_resources = ['vader_lexicon', 'punkt', 'stopwords', 'wordnet']\n",
    "    for resource in nlkt_resources:\n",
    "        try:\n",
    "            nltk.data.find(f'corpora/{resource}')\n",
    "            logging.info(f\"NLTK resource '{resource}' already exists.\")\n",
    "        except LookupError:\n",
    "            logging.info(f\"Downloading NLTK resource '{resource}'...\")\n",
    "            nltk.download(resource)\n",
    "\n",
    "\n",
    "def preprocess_text(text: str, lemmatizer: WordNetLemmatizer, stop_words: set) -> List[str]:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)      \n",
    "    text = re.sub(r'\\S+@\\S+', '', text)               \n",
    "    text = re.sub(r'<.*?>', '', text)                  \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'\\d+', '', text)                    \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens\n",
    "def load_glove_embeddings(cache_file: str = 'glove_cache.pkl') -> Dict[str, np.ndarray]:\n",
    "    if os.path.exists(cache_file):\n",
    "        logging.info(f\"Loading GloVe embeddings from cache: {cache_file}\")\n",
    "        try:\n",
    "            with open(cache_file, 'rb') as f:\n",
    "                embeddings_index = pickle.load(f)\n",
    "            logging.info(f\"Successfully loaded {len(embeddings_index)} word vectors from cache.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load cache file {cache_file}: {e}\")\n",
    "            raise\n",
    "    else:\n",
    "        logging.error(f\"Cache file not found at '{cache_file}'. Please ensure the cache exists.\")\n",
    "        raise FileNotFoundError(f\"Cache file '{cache_file}' does not exist.\")\n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "def get_document_embedding_glove(tokens: List[str], embeddings: Dict[str, np.ndarray], vector_size: int = 300) -> np.ndarray:\n",
    "    vectors = [embeddings[word] for word in tokens if word in embeddings]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "\n",
    "def evaluate_model(y_true: np.ndarray, y_pred: np.ndarray, model_name: str) -> None:\n",
    "    logging.info(f\"\\n{model_name} Classification Report:\")\n",
    "    report = classification_report(y_true, y_pred, target_names=['Negative', 'Positive'])\n",
    "    logging.info(report)\n",
    "\n",
    "\n",
    "def run_pipeline(data_path: str, glove_cache_path: str, output_dir: str = 'outputs') -> None:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_logger(os.path.join(output_dir, 'sentimentOutput.log'))\n",
    "    check_nltk()\n",
    "    logging.info(\"Sentiment Analysis Pipeline Started.\")\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    logging.info(\"Loading dataset...\")\n",
    "    data = pd.read_excel(data_path)\n",
    "    data = data.dropna(subset=['review_text'])\n",
    "    logging.info(f\"After dropping missing 'review_text', {data.shape[0]} samples remain.\")\n",
    "\n",
    "    data['sentiment'] = ['positive' if r >= 7 else 'negative' for r in data['rating']]\n",
    "    y = data['sentiment']\n",
    "    X = data['review_text']\n",
    "\n",
    "    logging.info(\"Preprocessing text data...\")\n",
    "    X_tokens = X.apply(lambda x: preprocess_text(x, lemmatizer, stop_words))\n",
    "    logging.info(\"Text preprocessing completed.\")\n",
    "\n",
    "    embeddings_index = load_glove_embeddings(cache_file=glove_cache_path)\n",
    "\n",
    "    logging.info(\"Creating document embeddings...\")\n",
    "    X_embedded = X_tokens.apply(lambda tokens: get_document_embedding_glove(tokens, embeddings_index))\n",
    "    X_embedded = np.vstack(X_embedded.values)\n",
    "    logging.info(f\"Document embeddings created with shape: {X_embedded.shape}\")\n",
    "\n",
    "    logging.info(\"Generating VADER sentiment scores...\")\n",
    "    vader_scores = X.apply(lambda x: sia.polarity_scores(x))\n",
    "    vader_scores_df = pd.DataFrame(list(vader_scores))\n",
    "    X_vader = vader_scores_df[['neg', 'neu', 'pos', 'compound']].values\n",
    "    logging.info(\"VADER sentiment scores generated.\")\n",
    "\n",
    "    logging.info(\"Integrating VADER scores with GloVe embeddings...\")\n",
    "    X_final = np.hstack((X_embedded, X_vader))\n",
    "    logging.info(f\"Final feature set shape before scaling: {X_final.shape}\")\n",
    "\n",
    "    logging.info(\"Applying StandardScaler to features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_final = scaler.fit_transform(X_final)\n",
    "    logging.info(\"Feature scaling applied.\")\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    logging.info(\"Labels encoded.\")\n",
    "\n",
    "    logging.info(\"Splitting dataset into training and testing sets...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_final, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "    logging.info(f\"Training set size: {X_train.shape[0]}\")\n",
    "    logging.info(f\"Testing set size: {X_test.shape[0]}\")\n",
    "\n",
    "    logging.info(\"Applying SMOTE to balance classes...\")\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "    logging.info(f\"After SMOTE: {X_train.shape[0]} training samples\")\n",
    "\n",
    "    models = {}\n",
    "    \n",
    "    models['LightGBM'] = LGBMClassifier(boosting_type='gbdt', learning_rate=0.1, max_depth=20, n_estimators=200, num_leaves=50, class_weight='balanced', random_state=42, verbose=-1)\n",
    "    \n",
    "    models['CatBoost'] = CatBoostClassifier(iterations=200, depth=10, learning_rate=0.1, l2_leaf_reg=3, class_weights=[1.0, 1.0], random_seed=42, verbose=0)\n",
    "    \n",
    "    models['XGBoost'] = XGBClassifier(scale_pos_weight=1.0, n_estimators=200, max_depth=10, learning_rate=0.1, subsample=0.8, colsample_bytree=0.8, objective='binary:logistic', use_label_encoder=False, eval_metric='logloss',random_state=4)\n",
    "    \n",
    "    models['BalancedRandomForest'] = BalancedRandomForestClassifier(n_estimators=200, max_depth=20, min_samples_split=2, min_samples_leaf=1, random_state=42)\n",
    "    \n",
    "    estimators = [('lgbm', models['LightGBM']), ('catboost', models['CatBoost'])]\n",
    "\n",
    "    models['StackingClassifier'] = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(), cv=5, n_jobs=-1)\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        logging.info(f\"\\nProcessing {model_name}...\")\n",
    "\n",
    "        logging.info(f\"Training {model_name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        logging.info(f\"{model_name} training completed.\")\n",
    "\n",
    "        logging.info(f\"Evaluating {model_name}...\")\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        else:\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_pred_proba = model.decision_function(X_test) if hasattr(model, 'decision_function') else y_pred\n",
    "        evaluate_model(y_test, y_pred, model_name)\n",
    "\n",
    "        logging.info(f\"Performing cross-validation for {model_name}...\")\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        cv_scores = cross_val_score(\n",
    "            model, X_final, y_encoded, cv=skf, scoring='f1_macro', n_jobs=-1\n",
    "        )\n",
    "        logging.info(f\"{model_name} Cross-Validated F1-Macro Scores: {cv_scores}\")\n",
    "        logging.info(f\"Average F1-Macro Score: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "    logging.info(\"\\nAll processes completed successfully!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_path = 'data/data.xlsx'          \n",
    "    glove_cache_path = 'glove_cache.pkl' \n",
    "    output_dir = 'outputs'                \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_logger(os.path.join(output_dir, 'sentimentOutput.log'))\n",
    "    check_nltk()\n",
    "    run_pipeline(data_path, glove_cache_path, output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
