{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3191aa0-9f11-4768-8f61-90161a23a2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 21:43:26,715 - INFO - Downloading NLTK resource 'vader_lexicon'...\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/abduladeeb/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "2024-12-08 21:43:26,716 - INFO - Downloading NLTK resource 'punkt'...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/abduladeeb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "2024-12-08 21:43:26,717 - INFO - NLTK resource 'stopwords' already exists.\n",
      "2024-12-08 21:43:26,718 - INFO - Downloading NLTK resource 'wordnet'...\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/abduladeeb/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "2024-12-08 21:43:26,719 - INFO - Downloading NLTK resource 'vader_lexicon'...\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/abduladeeb/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "2024-12-08 21:43:26,719 - INFO - Downloading NLTK resource 'punkt'...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/abduladeeb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "2024-12-08 21:43:26,720 - INFO - NLTK resource 'stopwords' already exists.\n",
      "2024-12-08 21:43:26,721 - INFO - Downloading NLTK resource 'wordnet'...\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/abduladeeb/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "2024-12-08 21:43:26,721 - INFO - Sentiment Analysis Pipeline Started.\n",
      "2024-12-08 21:43:26,725 - INFO - Loading dataset...\n",
      "2024-12-08 21:43:42,245 - INFO - After dropping missing 'review_text', 146393 samples remain.\n",
      "2024-12-08 21:43:42,259 - INFO - Preprocessing text data...\n",
      "2024-12-08 21:44:04,015 - INFO - Text preprocessing completed.\n",
      "2024-12-08 21:44:04,016 - INFO - Loading GloVe embeddings from cache: glove_cache.pkl\n",
      "2024-12-08 21:44:04,534 - INFO - Successfully loaded 400000 word vectors from cache.\n",
      "2024-12-08 21:44:04,535 - INFO - Creating document embeddings...\n",
      "2024-12-08 21:44:07,532 - INFO - Document embeddings created with shape: (146393, 300)\n",
      "2024-12-08 21:44:07,533 - INFO - Generating VADER sentiment scores...\n",
      "2024-12-08 21:44:44,878 - INFO - VADER sentiment scores generated.\n",
      "2024-12-08 21:44:44,878 - INFO - Integrating VADER scores with GloVe embeddings...\n",
      "2024-12-08 21:44:44,944 - INFO - Final feature set shape before scaling: (146393, 304)\n",
      "2024-12-08 21:44:44,944 - INFO - Applying StandardScaler to features...\n",
      "2024-12-08 21:44:45,192 - INFO - Feature scaling applied.\n",
      "2024-12-08 21:44:45,202 - INFO - Labels encoded.\n",
      "2024-12-08 21:44:45,202 - INFO - Splitting dataset into training and testing sets...\n",
      "2024-12-08 21:44:45,306 - INFO - Training set size: 117114\n",
      "2024-12-08 21:44:45,306 - INFO - Testing set size: 29279\n",
      "2024-12-08 21:44:45,307 - INFO - Applying SMOTE to balance classes...\n",
      "2024-12-08 21:44:45,934 - INFO - After SMOTE: 216024 training samples\n",
      "2024-12-08 21:44:45,934 - INFO - \n",
      "Processing LightGBM...\n",
      "2024-12-08 21:44:45,935 - INFO - Training LightGBM...\n",
      "2024-12-08 21:45:01,439 - INFO - LightGBM training completed.\n",
      "2024-12-08 21:45:01,440 - INFO - Evaluating LightGBM...\n",
      "2024-12-08 21:45:01,640 - INFO - \n",
      "LightGBM Classification Report:\n",
      "2024-12-08 21:45:01,664 - INFO -               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.32      0.50      0.39      2275\n",
      "    Positive       0.96      0.91      0.93     27004\n",
      "\n",
      "    accuracy                           0.88     29279\n",
      "   macro avg       0.64      0.71      0.66     29279\n",
      "weighted avg       0.91      0.88      0.89     29279\n",
      "\n",
      "2024-12-08 21:45:01,713 - INFO - Confusion matrix saved to outputs/plots/LightGBM_confusion_matrix.png.\n",
      "2024-12-08 21:45:01,758 - INFO - ROC curve saved to outputs/plots/LightGBM_roc_curve.png.\n",
      "2024-12-08 21:45:01,812 - INFO - Precision-Recall curve saved to outputs/plots/LightGBM_precision_recall_curve.png.\n",
      "2024-12-08 21:45:01,812 - INFO - Performing cross-validation for LightGBM...\n",
      "2024-12-08 21:45:57,730 - INFO - LightGBM Cross-Validated F1-Macro Scores: [0.67065969 0.66826212 0.66943119 0.66786574 0.66855026]\n",
      "2024-12-08 21:45:57,732 - INFO - Average F1-Macro Score: 0.6690 ± 0.0010\n",
      "2024-12-08 21:45:57,733 - INFO - \n",
      "Processing CatBoost...\n",
      "2024-12-08 21:45:57,733 - INFO - Training CatBoost...\n",
      "2024-12-08 21:46:53,557 - INFO - CatBoost training completed.\n",
      "2024-12-08 21:46:53,558 - INFO - Evaluating CatBoost...\n",
      "2024-12-08 21:46:57,876 - INFO - \n",
      "CatBoost Classification Report:\n",
      "2024-12-08 21:46:57,900 - INFO -               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.30      0.49      0.37      2275\n",
      "    Positive       0.95      0.91      0.93     27004\n",
      "\n",
      "    accuracy                           0.87     29279\n",
      "   macro avg       0.63      0.70      0.65     29279\n",
      "weighted avg       0.90      0.87      0.89     29279\n",
      "\n",
      "2024-12-08 21:46:57,953 - INFO - Confusion matrix saved to outputs/plots/CatBoost_confusion_matrix.png.\n",
      "2024-12-08 21:46:58,000 - INFO - ROC curve saved to outputs/plots/CatBoost_roc_curve.png.\n",
      "2024-12-08 21:46:58,066 - INFO - Precision-Recall curve saved to outputs/plots/CatBoost_precision_recall_curve.png.\n",
      "2024-12-08 21:46:58,081 - INFO - Performing cross-validation for CatBoost...\n",
      "2024-12-08 21:49:18,762 - INFO - CatBoost Cross-Validated F1-Macro Scores: [0.58596245 0.5898086  0.58485612 0.59269846 0.58532385]\n",
      "2024-12-08 21:49:18,765 - INFO - Average F1-Macro Score: 0.5877 ± 0.0030\n",
      "2024-12-08 21:49:18,765 - INFO - \n",
      "Processing XGBoost...\n",
      "2024-12-08 21:49:18,766 - INFO - Training XGBoost...\n",
      "/opt/anaconda3/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [21:49:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "2024-12-08 21:49:51,433 - INFO - XGBoost training completed.\n",
      "2024-12-08 21:49:51,434 - INFO - Evaluating XGBoost...\n",
      "2024-12-08 21:49:51,531 - INFO - \n",
      "XGBoost Classification Report:\n",
      "2024-12-08 21:49:51,556 - INFO -               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.37      0.39      0.38      2275\n",
      "    Positive       0.95      0.94      0.95     27004\n",
      "\n",
      "    accuracy                           0.90     29279\n",
      "   macro avg       0.66      0.67      0.66     29279\n",
      "weighted avg       0.90      0.90      0.90     29279\n",
      "\n",
      "2024-12-08 21:49:51,612 - INFO - Confusion matrix saved to outputs/plots/XGBoost_confusion_matrix.png.\n",
      "2024-12-08 21:49:51,660 - INFO - ROC curve saved to outputs/plots/XGBoost_roc_curve.png.\n",
      "2024-12-08 21:49:51,714 - INFO - Precision-Recall curve saved to outputs/plots/XGBoost_precision_recall_curve.png.\n",
      "2024-12-08 21:49:51,714 - INFO - Performing cross-validation for XGBoost...\n",
      "/opt/anaconda3/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [21:49:56] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [21:49:56] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [21:49:56] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [21:49:56] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [21:49:57] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "2024-12-08 21:51:55,527 - INFO - XGBoost Cross-Validated F1-Macro Scores: [0.59256602 0.58653649 0.58181639 0.58150284 0.5853937 ]\n",
      "2024-12-08 21:51:55,532 - INFO - Average F1-Macro Score: 0.5856 ± 0.0040\n",
      "2024-12-08 21:51:55,532 - INFO - \n",
      "Processing BalancedRandomForest...\n",
      "2024-12-08 21:51:55,533 - INFO - Training BalancedRandomForest...\n",
      "/opt/anaconda3/lib/python3.12/site-packages/imblearn/ensemble/_forest.py:577: FutureWarning: The default of `sampling_strategy` will change from `'auto'` to `'all'` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `'all'` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/imblearn/ensemble/_forest.py:589: FutureWarning: The default of `replacement` will change from `False` to `True` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `True` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/imblearn/ensemble/_forest.py:601: FutureWarning: The default of `bootstrap` will change from `True` to `False` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `False` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "2024-12-08 22:04:16,111 - INFO - BalancedRandomForest training completed.\n",
      "2024-12-08 22:04:16,112 - INFO - Evaluating BalancedRandomForest...\n",
      "2024-12-08 22:04:17,788 - INFO - \n",
      "BalancedRandomForest Classification Report:\n",
      "2024-12-08 22:04:17,810 - INFO -               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.27      0.41      0.33      2275\n",
      "    Positive       0.95      0.91      0.93     27004\n",
      "\n",
      "    accuracy                           0.87     29279\n",
      "   macro avg       0.61      0.66      0.63     29279\n",
      "weighted avg       0.90      0.87      0.88     29279\n",
      "\n",
      "2024-12-08 22:04:17,865 - INFO - Confusion matrix saved to outputs/plots/BalancedRandomForest_confusion_matrix.png.\n",
      "2024-12-08 22:04:17,913 - INFO - ROC curve saved to outputs/plots/BalancedRandomForest_roc_curve.png.\n",
      "2024-12-08 22:04:17,969 - INFO - Precision-Recall curve saved to outputs/plots/BalancedRandomForest_precision_recall_curve.png.\n",
      "2024-12-08 22:04:17,969 - INFO - Performing cross-validation for BalancedRandomForest...\n",
      "/opt/anaconda3/lib/python3.12/site-packages/imblearn/ensemble/_forest.py:577: FutureWarning: The default of `sampling_strategy` will change from `'auto'` to `'all'` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `'all'` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/imblearn/ensemble/_forest.py:589: FutureWarning: The default of `replacement` will change from `False` to `True` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `True` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/imblearn/ensemble/_forest.py:601: FutureWarning: The default of `bootstrap` will change from `True` to `False` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `False` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/imblearn/ensemble/_forest.py:577: FutureWarning: The default of `sampling_strategy` will change from `'auto'` to `'all'` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `'all'` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/imblearn/ensemble/_forest.py:589: FutureWarning: The default of `replacement` will change from `False` to `True` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `True` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/imblearn/ensemble/_forest.py:601: FutureWarning: The default of `bootstrap` will change from `True` to `False` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `False` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/imblearn/ensemble/_forest.py:577: FutureWarning: The default of `sampling_strategy` will change from `'auto'` to `'all'` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `'all'` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/imblearn/ensemble/_forest.py:589: FutureWarning: The default of `replacement` will change from `False` to `True` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `True` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/imblearn/ensemble/_forest.py:577: FutureWarning: The default of `sampling_strategy` will change from `'auto'` to `'all'` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `'all'` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/imblearn/ensemble/_forest.py:577: FutureWarning: The default of `sampling_strategy` will change from `'auto'` to `'all'` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `'all'` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/imblearn/ensemble/_forest.py:589: FutureWarning: The default of `replacement` will change from `False` to `True` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `True` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/imblearn/ensemble/_forest.py:601: FutureWarning: The default of `bootstrap` will change from `True` to `False` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `False` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/imblearn/ensemble/_forest.py:601: FutureWarning: The default of `bootstrap` will change from `True` to `False` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `False` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/imblearn/ensemble/_forest.py:589: FutureWarning: The default of `replacement` will change from `False` to `True` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `True` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/imblearn/ensemble/_forest.py:601: FutureWarning: The default of `bootstrap` will change from `True` to `False` in version 0.13. This change will follow the implementation proposed in the original paper. Set to `False` to silence this warning and adopt the future behaviour.\n",
      "  warn(\n",
      "2024-12-08 22:05:08,594 - INFO - BalancedRandomForest Cross-Validated F1-Macro Scores: [0.58004454 0.57966134 0.57611803 0.57690351 0.58463852]\n",
      "2024-12-08 22:05:08,595 - INFO - Average F1-Macro Score: 0.5795 ± 0.0030\n",
      "2024-12-08 22:05:08,596 - INFO - \n",
      "Processing StackingClassifier...\n",
      "2024-12-08 22:05:08,596 - INFO - Training StackingClassifier...\n",
      "2024-12-08 22:10:02,042 - INFO - StackingClassifier training completed.\n",
      "2024-12-08 22:10:02,044 - INFO - Evaluating StackingClassifier...\n",
      "2024-12-08 22:10:06,950 - INFO - \n",
      "StackingClassifier Classification Report:\n",
      "2024-12-08 22:10:06,999 - INFO -               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.32      0.47      0.38      2275\n",
      "    Positive       0.95      0.92      0.93     27004\n",
      "\n",
      "    accuracy                           0.88     29279\n",
      "   macro avg       0.64      0.69      0.66     29279\n",
      "weighted avg       0.90      0.88      0.89     29279\n",
      "\n",
      "2024-12-08 22:10:07,073 - INFO - Confusion matrix saved to outputs/plots/StackingClassifier_confusion_matrix.png.\n",
      "2024-12-08 22:10:07,150 - INFO - ROC curve saved to outputs/plots/StackingClassifier_roc_curve.png.\n",
      "2024-12-08 22:10:07,242 - INFO - Precision-Recall curve saved to outputs/plots/StackingClassifier_precision_recall_curve.png.\n",
      "2024-12-08 22:10:07,242 - INFO - Performing cross-validation for StackingClassifier...\n",
      "2024-12-08 22:29:04,320 - INFO - StackingClassifier Cross-Validated F1-Macro Scores: [0.65229058 0.64718182 0.64374852 0.64793488 0.64504102]\n",
      "2024-12-08 22:29:04,323 - INFO - Average F1-Macro Score: 0.6472 ± 0.0029\n",
      "2024-12-08 22:29:04,324 - INFO - \n",
      "Saving trained models...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ogging' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 339\u001b[0m\n\u001b[1;32m    336\u001b[0m output_logger(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentimentOutput.log\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    337\u001b[0m check_nltk()\n\u001b[0;32m--> 339\u001b[0m run_pipeline(data_path, glove_cache_path, output_dir)\n",
      "Cell \u001b[0;32mIn[9], line 319\u001b[0m, in \u001b[0;36mrun_pipeline\u001b[0;34m(data_path, glove_cache_path, output_dir)\u001b[0m\n\u001b[1;32m    317\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(models_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    318\u001b[0m     joblib\u001b[38;5;241m.\u001b[39mdump(model, model_path)\n\u001b[0;32m--> 319\u001b[0m     ogging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# Save scaler as well\u001b[39;00m\n\u001b[1;32m    321\u001b[0m scaler_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(models_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaler.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ogging' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import logging\n",
    "import pickle\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    StratifiedKFold\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "def output_logger(log_file: str='sentimentOutput.log')-> None:\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[logging.FileHandler(log_file), logging.StreamHandler()]\n",
    "    )\n",
    "def check_nltk() -> None:\n",
    "    nlkt_resources=['vader_lexicon','punkt','stopwords','wordnet']\n",
    "    for resource in nlkt_resources:\n",
    "        try:\n",
    "            nltk.data.find(f'corpora/{resource}')\n",
    "            logging.info(f\"NLTK resource '{resource}' already exists.\")\n",
    "        except LookupError:\n",
    "            logging.info(f\"Downloading NLTK resource '{resource}'...\")\n",
    "            nltk.download(resource)\n",
    "            \n",
    "def preprocess_text(text: str, lemmatizer: WordNetLemmatizer, stop_words: set) -> List[str]:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)       # Remove URLs\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)                # Remove emails\n",
    "    text = re.sub(r'<.*?>', '', text)                  # Remove HTML tags\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'\\d+', '', text)                    # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "def load_glove_embeddings(cache_file: str = 'glove_cache.pkl') -> Dict[str, np.ndarray]:\n",
    "    if os.path.exists(cache_file):\n",
    "        logging.info(f\"Loading GloVe embeddings from cache: {cache_file}\")\n",
    "        try:\n",
    "            with open(cache_file, 'rb') as f:\n",
    "                embeddings_index = pickle.load(f)\n",
    "            logging.info(f\"Successfully loaded {len(embeddings_index)} word vectors from cache.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load cache file {cache_file}: {e}\")\n",
    "            raise\n",
    "    else:\n",
    "        logging.error(f\"Cache file not found at '{cache_file}'. Please ensure the cache exists.\")\n",
    "        raise FileNotFoundError(f\"Cache file '{cache_file}' does not exist.\")\n",
    "\n",
    "    return embeddings_index\n",
    "\n",
    "def get_document_embedding_glove(tokens: List[str],embeddings: Dict[str, np.ndarray],vector_size: int = 300) -> np.ndarray:\n",
    "    vectors = [embeddings[word] for word in tokens if word in embeddings]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "#          Evaluation             #\n",
    "\n",
    "def plot_confusion_matrix_custom(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    title: str,\n",
    "    save_path: str = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Negative', 'Positive'],\n",
    "                yticklabels=['Negative', 'Positive'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(title)\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        logging.info(f\"Confusion matrix saved to {save_path}.\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_roc_curve_custom(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred_proba: np.ndarray,\n",
    "    title: str,\n",
    "    save_path: str = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plots the ROC curve.\n",
    "    \"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        logging.info(f\"ROC curve saved to {save_path}.\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_precision_recall_curve_custom(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred_proba: np.ndarray,\n",
    "    title: str,\n",
    "    save_path: str = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plots the Precision-Recall curve.\n",
    "    \"\"\"\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)\n",
    "    average_precision = average_precision_score(y_true, y_pred_proba)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(recall, precision, color='b', label=f'AP = {average_precision:.2f}')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='lower left')\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        logging.info(f\"Precision-Recall curve saved to {save_path}.\")\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_model(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    y_pred_proba: np.ndarray,\n",
    "    model_name: str,\n",
    "    output_dir: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Evaluates the model by printing classification report and plotting evaluation curves.\n",
    "    \"\"\"\n",
    "    logging.info(f\"\\n{model_name} Classification Report:\")\n",
    "    report = classification_report(y_true, y_pred, target_names=['Negative', 'Positive'])\n",
    "    logging.info(report)\n",
    "    \n",
    "    # Create output directories if they don't exist\n",
    "    plots_dir = os.path.join(output_dir, 'plots')\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    \n",
    "    # Plot Confusion Matrix\n",
    "    plot_confusion_matrix_custom(\n",
    "        y_true, y_pred, \n",
    "        title=f\"{model_name} Confusion Matrix\",\n",
    "        save_path=os.path.join(plots_dir, f\"{model_name}_confusion_matrix.png\")\n",
    "    )\n",
    "    \n",
    "    # Plot ROC Curve\n",
    "    plot_roc_curve_custom(\n",
    "        y_true, y_pred_proba, \n",
    "        title=f\"{model_name} ROC Curve\",\n",
    "        save_path=os.path.join(plots_dir, f\"{model_name}_roc_curve.png\")\n",
    "    )\n",
    "    \n",
    "    # Plot Precision-Recall Curve\n",
    "    plot_precision_recall_curve_custom(\n",
    "        y_true, y_pred_proba, \n",
    "        title=f\"{model_name} Precision-Recall Curve\",\n",
    "        save_path=os.path.join(plots_dir, f\"{model_name}_precision_recall_curve.png\")\n",
    "    )\n",
    "\n",
    "def run_pipeline(data_path: str, glove_cache_path: str, output_dir: str = 'outputs') -> None:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_logger(os.path.join(output_dir, 'sentimentOutput.log'))\n",
    "    check_nltk()\n",
    "    logging.info(\"Sentiment Analysis Pipeline Started.\")\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    logging.info(\"Loading dataset...\")\n",
    "    data = pd.read_excel(data_path)\n",
    "    data = data.dropna(subset=['review_text'])\n",
    "    logging.info(f\"After dropping missing 'review_text', {data.shape[0]} samples remain.\")\n",
    "    \n",
    "    # Define sentiment based on rating\n",
    "    data['sentiment'] = ['positive' if r >= 7 else 'negative' for r in data['rating']]\n",
    "    y = data['sentiment']\n",
    "    X = data['review_text']\n",
    "\n",
    "    logging.info(\"Preprocessing text data...\")\n",
    "    X_tokens = X.apply(lambda x: preprocess_text(x, lemmatizer, stop_words))\n",
    "    logging.info(\"Text preprocessing completed.\")\n",
    "\n",
    "    embeddings_index = load_glove_embeddings(cache_file=glove_cache_path)\n",
    "    \n",
    "    logging.info(\"Creating document embeddings...\")\n",
    "    X_embedded = X_tokens.apply(lambda tokens: get_document_embedding_glove(tokens, embeddings_index))\n",
    "    X_embedded = np.vstack(X_embedded.values)\n",
    "    logging.info(f\"Document embeddings created with shape: {X_embedded.shape}\")\n",
    "    \n",
    "    logging.info(\"Generating VADER sentiment scores...\")\n",
    "    vader_scores = X.apply(lambda x: sia.polarity_scores(x))\n",
    "    vader_scores_df = pd.DataFrame(list(vader_scores))\n",
    "    X_vader = vader_scores_df[['neg', 'neu', 'pos', 'compound']].values\n",
    "    logging.info(\"VADER sentiment scores generated.\")\n",
    "\n",
    "    logging.info(\"Integrating VADER scores with GloVe embeddings...\")\n",
    "    X_final = np.hstack((X_embedded, X_vader))\n",
    "    logging.info(f\"Final feature set shape before scaling: {X_final.shape}\")\n",
    "\n",
    "    logging.info(\"Applying StandardScaler to features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_final = scaler.fit_transform(X_final)\n",
    "    logging.info(\"Feature scaling applied.\")\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    logging.info(\"Labels encoded.\")\n",
    "\n",
    "    logging.info(\"Splitting dataset into training and testing sets...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_final, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "    logging.info(f\"Training set size: {X_train.shape[0]}\")\n",
    "    logging.info(f\"Testing set size: {X_test.shape[0]}\")\n",
    "\n",
    "    logging.info(\"Applying SMOTE to balance classes...\")\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "    logging.info(f\"After SMOTE: {X_train.shape[0]} training samples\")\n",
    "\n",
    "    models = {}\n",
    "    models['LightGBM'] = LGBMClassifier(boosting_type='gbdt', learning_rate=0.1, max_depth=20, n_estimators=200, num_leaves=50, class_weight='balanced', random_state=42, verbose=-1)\n",
    "    \n",
    "    models['CatBoost'] = CatBoostClassifier(iterations=200, depth=10, learning_rate=0.1, l2_leaf_reg=3, class_weights=[1.0, 1.0], random_seed=42, verbose=0)\n",
    "    \n",
    "    models['XGBoost'] = XGBClassifier(scale_pos_weight=1.0, n_estimators=200, max_depth=10, learning_rate=0.1, subsample=0.8, colsample_bytree=0.8, objective='binary:logistic', use_label_encoder=False, eval_metric='logloss',random_state=4)\n",
    "    \n",
    "    models['BalancedRandomForest'] = BalancedRandomForestClassifier(n_estimators=200, max_depth=20, min_samples_split=2, min_samples_leaf=1, random_state=42)\n",
    "    \n",
    "    estimators = [('lgbm', models['LightGBM']), ('catboost', models['CatBoost'])]\n",
    "\n",
    "    models['StackingClassifier'] = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(), cv=5, n_jobs=-1)\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        logging.info(f\"\\nProcessing {model_name}...\")\n",
    "        \n",
    "\n",
    "        logging.info(f\"Training {model_name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        logging.info(f\"{model_name} training completed.\")\n",
    "        \n",
    "        logging.info(f\"Evaluating {model_name}...\")\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        else:\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_pred_proba = model.decision_function(X_test) if hasattr(model, 'decision_function') else y_pred\n",
    "        evaluate_model(y_test, y_pred, y_pred_proba, model_name, output_dir)\n",
    "        \n",
    "        logging.info(f\"Performing cross-validation for {model_name}...\")\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        cv_scores = cross_val_score(model, X_final, y_encoded, cv=skf, scoring='f1_macro',n_jobs=-1)\n",
    "        logging.info(f\"{model_name} Cross-Validated F1-Macro Scores: {cv_scores}\")\n",
    "        logging.info(f\"Average F1-Macro Score: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "    logging.info(\"\\nSaving trained models...\")\n",
    "    models_dir = os.path.join(output_dir, 'models')\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        model_path = os.path.join(models_dir, f\"{model_name}.pkl\")\n",
    "        joblib.dump(model, model_path)\n",
    "        ogging.info(f\"{model_name} saved to {model_path}.\")\n",
    "    scaler_path = os.path.join(models_dir, \"scaler.pkl\")\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    logging.info(f\"Scaler object saved.\")\n",
    "    logging.info(\"\\nAll processes completed successfully!\")\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    data_path = 'data/data.xlsx'            \n",
    "    glove_cache_path = 'glove_cache.pkl'   \n",
    "    output_dir = 'outputs'                 \n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_logger(os.path.join(output_dir, 'sentimentOutput.log'))\n",
    "    check_nltk()\n",
    "    \n",
    "    run_pipeline(data_path, glove_cache_path, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caef34b3-8265-4aa7-94d6-9a6de7657a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 22:29:35,262 - INFO - Downloading NLTK resource 'vader_lexicon'...\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/abduladeeb/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "2024-12-08 22:29:35,401 - INFO - Downloading NLTK resource 'punkt'...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/abduladeeb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "2024-12-08 22:29:35,447 - INFO - NLTK resource 'stopwords' already exists.\n",
      "2024-12-08 22:29:35,449 - INFO - Downloading NLTK resource 'wordnet'...\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/abduladeeb/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "2024-12-08 22:29:35,475 - INFO - Downloading NLTK resource 'vader_lexicon'...\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/abduladeeb/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "2024-12-08 22:29:35,476 - INFO - Downloading NLTK resource 'punkt'...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/abduladeeb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "2024-12-08 22:29:35,477 - INFO - NLTK resource 'stopwords' already exists.\n",
      "2024-12-08 22:29:35,478 - INFO - Downloading NLTK resource 'wordnet'...\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/abduladeeb/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "2024-12-08 22:29:35,479 - INFO - Sentiment Analysis Pipeline Started.\n",
      "2024-12-08 22:29:35,489 - INFO - Loading dataset...\n",
      "2024-12-08 22:29:53,949 - INFO - After dropping missing 'review_text', 146393 samples remain.\n",
      "2024-12-08 22:29:53,963 - INFO - Preprocessing text data...\n",
      "2024-12-08 22:30:16,210 - INFO - Text preprocessing completed.\n",
      "2024-12-08 22:30:16,211 - INFO - Loading GloVe embeddings from cache: glove_cache.pkl\n",
      "2024-12-08 22:30:17,046 - INFO - Successfully loaded 400000 word vectors from cache.\n",
      "2024-12-08 22:30:17,047 - INFO - Creating document embeddings...\n",
      "2024-12-08 22:30:19,615 - INFO - Document embeddings created with shape: (146393, 300)\n",
      "2024-12-08 22:30:19,616 - INFO - Generating VADER sentiment scores...\n",
      "2024-12-08 22:30:58,083 - INFO - VADER sentiment scores generated.\n",
      "2024-12-08 22:30:58,084 - INFO - Integrating VADER scores with GloVe embeddings...\n",
      "2024-12-08 22:30:58,180 - INFO - Final feature set shape before scaling: (146393, 304)\n",
      "2024-12-08 22:30:58,181 - INFO - Applying StandardScaler to features...\n",
      "2024-12-08 22:30:58,509 - INFO - Feature scaling applied.\n",
      "2024-12-08 22:30:58,519 - INFO - Labels encoded.\n",
      "2024-12-08 22:30:58,519 - INFO - Splitting dataset into training and testing sets...\n",
      "2024-12-08 22:30:58,622 - INFO - Training set size: 117114\n",
      "2024-12-08 22:30:58,622 - INFO - Testing set size: 29279\n",
      "2024-12-08 22:30:58,623 - INFO - Applying SMOTE to balance classes...\n",
      "2024-12-08 22:30:59,435 - INFO - After SMOTE: 216024 training samples\n",
      "2024-12-08 22:30:59,436 - INFO - \n",
      "Processing LightGBM...\n",
      "2024-12-08 22:30:59,436 - INFO - Training LightGBM...\n",
      "2024-12-08 22:31:15,323 - INFO - LightGBM training completed.\n",
      "2024-12-08 22:31:15,324 - INFO - Evaluating LightGBM...\n",
      "2024-12-08 22:31:15,523 - INFO - \n",
      "LightGBM Classification Report:\n",
      "2024-12-08 22:31:15,546 - INFO -               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.32      0.50      0.39      2275\n",
      "    Positive       0.96      0.91      0.93     27004\n",
      "\n",
      "    accuracy                           0.88     29279\n",
      "   macro avg       0.64      0.71      0.66     29279\n",
      "weighted avg       0.91      0.88      0.89     29279\n",
      "\n",
      "2024-12-08 22:31:15,546 - INFO - Performing cross-validation for LightGBM...\n",
      "2024-12-08 22:32:12,806 - INFO - LightGBM Cross-Validated F1-Macro Scores: [0.67065969 0.66826212 0.66943119 0.66786574 0.66855026]\n",
      "2024-12-08 22:32:12,808 - INFO - Average F1-Macro Score: 0.6690 ± 0.0010\n",
      "2024-12-08 22:32:12,809 - INFO - \n",
      "Processing CatBoost...\n",
      "2024-12-08 22:32:12,809 - INFO - Training CatBoost...\n",
      "2024-12-08 22:33:10,207 - INFO - CatBoost training completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import logging\n",
    "import pickle\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    StratifiedKFold\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "\n",
    "def output_logger(log_file: str = 'sentimentOutput.log') -> None:\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[logging.FileHandler(log_file), logging.StreamHandler()]\n",
    "    )\n",
    "\n",
    "\n",
    "def check_nltk() -> None:\n",
    "    nlkt_resources = ['vader_lexicon', 'punkt', 'stopwords', 'wordnet']\n",
    "    for resource in nlkt_resources:\n",
    "        try:\n",
    "            nltk.data.find(f'corpora/{resource}')\n",
    "            logging.info(f\"NLTK resource '{resource}' already exists.\")\n",
    "        except LookupError:\n",
    "            logging.info(f\"Downloading NLTK resource '{resource}'...\")\n",
    "            nltk.download(resource)\n",
    "\n",
    "\n",
    "def preprocess_text(text: str, lemmatizer: WordNetLemmatizer, stop_words: set) -> List[str]:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)       # Remove URLs\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)                # Remove emails\n",
    "    text = re.sub(r'<.*?>', '', text)                  # Remove HTML tags\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'\\d+', '', text)                    # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def load_glove_embeddings(cache_file: str = 'glove_cache.pkl') -> Dict[str, np.ndarray]:\n",
    "    if os.path.exists(cache_file):\n",
    "        logging.info(f\"Loading GloVe embeddings from cache: {cache_file}\")\n",
    "        try:\n",
    "            with open(cache_file, 'rb') as f:\n",
    "                embeddings_index = pickle.load(f)\n",
    "            logging.info(f\"Successfully loaded {len(embeddings_index)} word vectors from cache.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load cache file {cache_file}: {e}\")\n",
    "            raise\n",
    "    else:\n",
    "        logging.error(f\"Cache file not found at '{cache_file}'. Please ensure the cache exists.\")\n",
    "        raise FileNotFoundError(f\"Cache file '{cache_file}' does not exist.\")\n",
    "\n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "def get_document_embedding_glove(tokens: List[str], embeddings: Dict[str, np.ndarray], vector_size: int = 300) -> np.ndarray:\n",
    "    vectors = [embeddings[word] for word in tokens if word in embeddings]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "\n",
    "def evaluate_model(y_true: np.ndarray, y_pred: np.ndarray, model_name: str) -> None:\n",
    "    logging.info(f\"\\n{model_name} Classification Report:\")\n",
    "    report = classification_report(y_true, y_pred, target_names=['Negative', 'Positive'])\n",
    "    logging.info(report)\n",
    "\n",
    "\n",
    "def run_pipeline(data_path: str, glove_cache_path: str, output_dir: str = 'outputs') -> None:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_logger(os.path.join(output_dir, 'sentimentOutput.log'))\n",
    "    check_nltk()\n",
    "    logging.info(\"Sentiment Analysis Pipeline Started.\")\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    logging.info(\"Loading dataset...\")\n",
    "    data = pd.read_excel(data_path)\n",
    "    data = data.dropna(subset=['review_text'])\n",
    "    logging.info(f\"After dropping missing 'review_text', {data.shape[0]} samples remain.\")\n",
    "\n",
    "    data['sentiment'] = ['positive' if r >= 7 else 'negative' for r in data['rating']]\n",
    "    y = data['sentiment']\n",
    "    X = data['review_text']\n",
    "\n",
    "    logging.info(\"Preprocessing text data...\")\n",
    "    X_tokens = X.apply(lambda x: preprocess_text(x, lemmatizer, stop_words))\n",
    "    logging.info(\"Text preprocessing completed.\")\n",
    "\n",
    "    embeddings_index = load_glove_embeddings(cache_file=glove_cache_path)\n",
    "\n",
    "    logging.info(\"Creating document embeddings...\")\n",
    "    X_embedded = X_tokens.apply(lambda tokens: get_document_embedding_glove(tokens, embeddings_index))\n",
    "    X_embedded = np.vstack(X_embedded.values)\n",
    "    logging.info(f\"Document embeddings created with shape: {X_embedded.shape}\")\n",
    "\n",
    "    logging.info(\"Generating VADER sentiment scores...\")\n",
    "    vader_scores = X.apply(lambda x: sia.polarity_scores(x))\n",
    "    vader_scores_df = pd.DataFrame(list(vader_scores))\n",
    "    X_vader = vader_scores_df[['neg', 'neu', 'pos', 'compound']].values\n",
    "    logging.info(\"VADER sentiment scores generated.\")\n",
    "\n",
    "    logging.info(\"Integrating VADER scores with GloVe embeddings...\")\n",
    "    X_final = np.hstack((X_embedded, X_vader))\n",
    "    logging.info(f\"Final feature set shape before scaling: {X_final.shape}\")\n",
    "\n",
    "    logging.info(\"Applying StandardScaler to features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_final = scaler.fit_transform(X_final)\n",
    "    logging.info(\"Feature scaling applied.\")\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    logging.info(\"Labels encoded.\")\n",
    "\n",
    "    logging.info(\"Splitting dataset into training and testing sets...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_final, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "    logging.info(f\"Training set size: {X_train.shape[0]}\")\n",
    "    logging.info(f\"Testing set size: {X_test.shape[0]}\")\n",
    "\n",
    "    logging.info(\"Applying SMOTE to balance classes...\")\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "    logging.info(f\"After SMOTE: {X_train.shape[0]} training samples\")\n",
    "\n",
    "    models = {}\n",
    "    models['LightGBM'] = LGBMClassifier(\n",
    "        boosting_type='gbdt',\n",
    "        learning_rate=0.1,\n",
    "        max_depth=20,\n",
    "        n_estimators=200,\n",
    "        num_leaves=50,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    )\n",
    "\n",
    "    models['CatBoost'] = CatBoostClassifier(\n",
    "        iterations=200,\n",
    "        depth=10,\n",
    "        learning_rate=0.1,\n",
    "        l2_leaf_reg=3,\n",
    "        class_weights=[1.0, 1.0],\n",
    "        random_seed=42,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    models['XGBoost'] = XGBClassifier(\n",
    "        scale_pos_weight=1.0,\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective='binary:logistic',\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        random_state=4\n",
    "    )\n",
    "\n",
    "    models['BalancedRandomForest'] = BalancedRandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=20,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    estimators = [('lgbm', models['LightGBM']), ('catboost', models['CatBoost'])]\n",
    "\n",
    "    models['StackingClassifier'] = StackingClassifier(\n",
    "        estimators=estimators,\n",
    "        final_estimator=LogisticRegression(),\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        logging.info(f\"\\nProcessing {model_name}...\")\n",
    "\n",
    "        logging.info(f\"Training {model_name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        logging.info(f\"{model_name} training completed.\")\n",
    "\n",
    "        logging.info(f\"Evaluating {model_name}...\")\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        else:\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_pred_proba = model.decision_function(X_test) if hasattr(model, 'decision_function') else y_pred\n",
    "        evaluate_model(y_test, y_pred, model_name)\n",
    "\n",
    "        logging.info(f\"Performing cross-validation for {model_name}...\")\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        cv_scores = cross_val_score(\n",
    "            model, X_final, y_encoded, cv=skf, scoring='f1_macro', n_jobs=-1\n",
    "        )\n",
    "        logging.info(f\"{model_name} Cross-Validated F1-Macro Scores: {cv_scores}\")\n",
    "        logging.info(f\"Average F1-Macro Score: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "    logging.info(\"\\nSaving trained models...\")\n",
    "    models_dir = os.path.join(output_dir, 'models')\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        model_path = os.path.join(models_dir, f\"{model_name}.pkl\")\n",
    "        joblib.dump(model, model_path)\n",
    "        logging.info(f\"{model_name} saved to {model_path}.\")\n",
    "    scaler_path = os.path.join(models_dir, \"scaler.pkl\")\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    logging.info(f\"Scaler object saved.\")\n",
    "    logging.info(\"\\nAll processes completed successfully!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_path = 'data/data.xlsx'          \n",
    "    glove_cache_path = 'glove_cache.pkl' \n",
    "    output_dir = 'outputs'                \n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_logger(os.path.join(output_dir, 'sentimentOutput.log'))\n",
    "    check_nltk()\n",
    "\n",
    "    run_pipeline(data_path, glove_cache_path, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd754df6-584f-4043-9b5f-ca88eb1b98e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
